\chapter{Background}
\label{cp:Background}

\section{Image data}
An image may be defined as a two-dimensional function. $f(x,y)$, where $x$ and $y$ are spatial (plane) coordinates, and the amplitude of $f$ at any pair of coordinates $(x,y)$ is called the intensity or grey level of the image at that point. When $x$, $y$, and the intensity values of $f$ are all finite, discrete quantities, which is a discrete signal, we call the image a digital image~\cite{gonzalez2002digital}. The intensity of an 8-bit grayscale image takes the grey value of integer from 0 (black) to 255 (white). An RGB image, considered in this study, contains three 8-bit images, each representing R, G, and B frames, respectively. 

\section{Discrete wavelet decomposition}

The Discrete wavelet transform (DWT) foundations go back to 1976 when Croiser, Esteban, and Galand devised a technique to decompose discrete-time signals\nocite{croisier1976perfect}. In 1983, Burt ~\nocite{Burt1983TheLP}defined a very similar technique to subband coding and named it pyramidal coding, also known as multiresolution analysis. Later in 1989, Vetterli and Le Gall~\nocite{vetterli1989perfect} made some improvements to the subband coding scheme, removing the existing redundancy in the pyramidal coding scheme.

DWT is any wavelet transform for which the wavelets are discretely sampled. As with other wavelet transforms, a key advantage over Fourier transforms (FT) is temporal resolution: it captures both frequency and location information (location in time), Fig.~\ref{fig:sin_signal} show that a Sinusoidal signal (S) is composed of four frequency components at 30 Hz, 20 Hz, 10 Hz and 5 Hz. Fig.~\ref{fig:sin_tf} and Fig.~\ref{fig:sin_cwt} show the transformed signal of Fourier transform and Continuous Wavelet Transform, respectively. 

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/signal_sin.jpg}
\caption{Sinusoidal signal composed of four frequency components at 30 Hz, 20 Hz, 10 Hz and 5 Hz}
\label{fig:sin_signal}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=6cm]{images/ft_sin.jpg}
\caption{Sinusoidal signal after Fourier Transform}
\label{fig:sin_tf}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=6cm]{images/dwt_sin.png}
\caption{Sinusoidal signal after Continuous Wavelet Transform}
\label{fig:sin_cwt}
\end{minipage}
\end{figure}



% three pics in one line
\begin{comment}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/ft_sin.jpg}
\caption{Sinusoidal signal after Fourier Transform}
\label{fig:sin_tf}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/dwt_sin.png}
\caption{Sinusoidal signal after Continuous Wavelet Transform}
\label{fig:sin_cwt}
\end{figure}


\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=4cm]{images/signal_sin.jpg}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=4cm]{images/ft_sin.jpg}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=4cm]{images/dwt_sin.png}
\end{minipage}
\end{figure}
\end{comment}


In Fourier transforms, we cannot know what spectral component exists at any given time instant. The best we can do is to investigate what spectral components exist at any given interval of time. This is a problem of resolution, and it is the main reason we use wavelet transform (WT) instead of FT. FT gives a fixed resolution at all times, whereas WT gives a variable resolution as follows:

Higher frequencies are better resolved in time, and lower frequencies are better resolved in frequency. This means that a particular high-frequency component can be located better in the time domain (with less relative error) than a low-frequency component. On the contrary, a low-frequency component can be better resolved in the frequency domain than a high-frequency component~\cite{polikar1996wavelet}.

We can visualize the resolution problem at Figure ~\ref{fig:resolution}-~\ref{fig:resolution_frequency}:

\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=6cm]{images/resolution_1.JPG}
\caption{Illustration of signals with different frequency in time domain}
\label{fig:resolution}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=6cm]{images/resolution_blank2.PNG}
\caption{Illustration of different frequency components resolved in frequency domain}
\label{fig:resolution_frequency}
\end{minipage}
\end{figure}


In Figure~\ref{fig:resolution} the top row shows that at higher frequencies, we have more samples corresponding to smaller intervals of time. In other words, higher frequencies can be resolved better in time, which mean we can easily find out higher frequency signal in the time domain. As the frequency decrease, there are fewer points to characterize the signal. Therefore, low frequencies have poor resolution in time domain. 

In Figure~\ref{fig:resolution_frequency}, the time resolution of the signal works the same as shown in Figure~\ref{fig:resolution}, but now, the frequency information has different resolutions at every stage too. We can note that the space between subsequent frequency components increases as frequency increases. Thus lower frequencies are better resolved in frequency, whereas higher frequencies are not.

Multiresolution analysis (MRA), also known as Discrete Wavelet Transform, is a framework proposed to analyzes the signal at different frequencies with different resolutions. MRA is designed to give reasonable time resolution at high frequencies and good frequency resolution at low frequencies. This approach makes sense, especially when the signal at hand has high-frequency components for short durations and low-frequency components for long durations. Fortunately, most signals (include image signal in our case) that are encountered in practical applications are often of this type.

The DWT procedure starts with passing the discrete signal $x[n]$ through a half band digital lowpass filter with impulse response $h[n]$. Filtering a signal corresponds to the mathematical operation of convolution of the signal with the impulse response of the filter. The convolution operation in discrete time is defined as follows:

\begin{equation}
y[n]=x[n] * h[n]=\sum_{k=-\infty}^{\infty} x[k] \cdot h[n-k]\,.
\label{equ:convolude}
\end{equation}

After passing the signal through a half-band lowpass filter, signals with frequencies larger than $f/2$ ($f$ is the maximum frequency of the filtered signal) have disappeared. Since half the signal frequencies have now been removed, half the samples can be discarded according to Nyquist’s rule~\cite{shannon1949communication}, which states if a function $x(t)$ contains no frequencies higher than $B$ hertz, for a given sample rate $f_{s}$, perfect reconstruction is guaranteed possible for a bandlimit  $f_{s}/2>B$, which can be achieved by subsampling the signal by two, and the signal will then have half the number of points. The resolution of this signal, which is a measure of the amount of detailed information of signal, is halved by the lowpass filter operation. While the scale, which is the inverse of frequency, of the signal is now doubled. 

In order to better illustrate the parameter scale, we can imagine it as the scale used in maps: high scales in case of map correspond to a non-detailed global view (of the signal), and low scales correspond to a detailed local view. Similarly, in terms of frequency, low frequencies (high scales) correspond to global information of a signal (that usually spans the entire signal), whereas high frequencies (low scales) correspond to detailed information of a hidden pattern in the signal. 

One thing worth paying particular attention to is that the subsampling operation after filtering does not affect the resolution since removing half of the spectral components from the signal (in which the number of samples stays unchanged) makes half the number of samples redundant. Thus, half the samples can be discarded without any loss of information. In summary, the lowpass filtering halves the resolution but leaves the scale unchanged. The signal is then subsampled by two since half of the number of samples is redundant, which doubled the scale.

This procedure can mathematically be expressed as

\begin{equation}
y[n]=\sum_{k=-\infty}^{\infty} x[k] \cdot h[2 n-k]\,.
\label{equ:subband}
\end{equation}

For the principle of decomposes the signal with DWT, we can draw the following conclusions: The DWT analyzes the signal at different frequency bands with different resolutions by decomposing the signal, which is achieved by filtering the time domain signal successively with highpass and lowpass. The signal filtered by highpass and lowpass constitutes one level of decomposition and can
mathematically be expressed as follows:

\begin{equation}
\begin{aligned}
&y_{\text {low }}[n]=\sum_{k=-\infty}^{\infty} x[k] g[2 n-k]\,, \\
&y_{\text {high }}[n]=\sum_{k=-\infty}^{\infty} x[k] h[2 n-k]\,,
\end{aligned}
\end{equation}

where $y_{\text {high }}[n]$ and $y_{\text {low }}[n]$ are the outputs of the highpass and lowpass filters after subsampling by two, respectively, which correspond to detailed information and coarse approximation.

After decomposition, the time resolution of the signal is halved, while the frequency resolution is doubled, this is because only half of the number of samples can represent the entire signal, and the frequency band of the signal only spans half of the previous frequency band. The above process is also called subband coding and can be repeated for further decomposition. At each level, filtering and subsampling will cause the number of samples to be halved (thus, the time resolution is halved), and the spanned frequency band is halved (thus, the frequency resolution is doubled). This procedure is shown in Figure ~\ref{fig:dwt1}, in which $x[n]$ is the original signal, and $h[n]$ and $g[n]$ are lowpass and highpass filters, respectively. $f$ represents the bandwidth of each level. 

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{images/dwt1.PNG}
\caption[DWT decomposition algorithms]{A schematic illustration of DWT decomposition. At the first beginning, the signal is decomposed into different coefficients by the highpass and lowpass filters, at each subsequent level the result of the lowpass filter is again decomposed  by the highpass and lowpass filters, until the final level is decomposed.}
\label{fig:dwt1}
\end{figure}





\section{Control chart}

Statistical process control (SPC) is a powerful collection of problem-solving
tools useful in achieving process stability and improving capability through the reduction of variability.
SPC is one of the greatest technological developments of the twentieth century because it is based on sound underlying principles, is easy to use, has a significant impact, and can be applied to any process~\cite{montgomery2020introduction}. Its seven major tools are
\begin{enumerate}
    \item Histogram or stem-and-leaf plot
    \item Check sheet
    \item Pareto chart
    \item Cause-and-effect diagram
    \item Defect concentration diagram
    \item Scatter diagram
    \item Control chart.
\end{enumerate}

The control chart is probably the easiest yet effective tool to analyze the process stability of these seven tools. To best understand the concepts of the Control chart, the classification of the control chart needs to be clarified. Depending on the number of process characteristics to be monitored, there are two primary control charts. The first, referred to as a univariate control chart, is a graphical chart of one quality characteristic, in which one is interested in monitoring changes in the parameter of an underlying univariate distribution
over time. The second is a multivariate control chart, a graphical chart of a statistic that fuses more than one quality characteristic to monitor simultaneous changes in the parameter
vector of an underlying multivariate distribution over time.


More specifically, the control chart is a graphical display, which plots the value of the quality characteristic that has been measured or calculated from a sample versus the sample number or versus time. Normally there are three lines in a control chart: a centerline corresponding to the mean value for the in-control process. Two other horizontal lines are called the upper control limit (UCL) and the lower control limit (LCL). These control limits are chosen so that almost all data points will fall within these limits as long as the process remains in control. A typical control chart is shown in Figure~\ref{fig:control_chart}.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/control_chart.PNG}
\caption[A typical control chart.]{A schematic illustration of the control chart. A center line that correspond to the mean value for the in-control process. Two other horizontal lines, called the upper control limit (UCL) and the lower control limit (LCL). The scope between the two lines indicate the in-control area of process. Figure is adapted from Montgomery (2020)}
\label{fig:control_chart}
\end{figure}

\subsection{Shewhart $\bar{X}$ control chart}
In statistical quality control, the $\bar{X}$ and s chart is a type of control chart used to monitor variables data when samples are collected at regular intervals from a business or industrial process ~\cite{heckert2002handbook}. 

In general, $\bar{X}$ and s control chart has following advantages:
\begin{enumerate}
    \item The sample size (n) is relatively large $(n>10)$.
    \item The sample size is variable.
\end{enumerate}

The chart consists of two individual control charts: One ($\bar{X}$ control chart) to monitor the process mean and the other (s control chart) to monitor the process standard deviation.

During the 1920s, Dr. Walter A. Shewhart first proposed a general model for control charts as follows~\cite{adams1999manufacturing}:

Let $s$ be a sample statistic that measures some continuously varying quality characteristic of interest (e.g., diameter), and suppose that the mean of $s$ is $\mu_{s}$, with a standard deviation of $\sigma_{s}$. Then the centerline (CL), the upper control limit (UCL), and the lower control limit (LCL) are


\begin{equation}
    U C L=\mu_{s}+k \sigma_{s}\,,
    \label{equ:UCL}
\end{equation}

\begin{equation}
    C L=\mu_{s}\,,
    \label{equ:CL}
\end{equation}

\begin{equation}
    U C L=\mu_{s}-k \sigma_{s}\,,
    \label{equ:LCL}
\end{equation}

where $k$ is the distance of the control limits from the centerline, expressed in standard deviation units. When $k$ is set to 3, we speak of 3-sigma control charts. Historically, $k=3$ has become an accepted standard in the industry.

The centerline is the process mean, which is unknown, so as the $\sigma$. We need to replace them with some confident values. e.g., the average of all the data and the average standard deviation, respectively. 

There exist two distinct phases of the control chart
~\cite{bersimis2007multivariate}:

\begin{itemize}
\item Phase I: charts are used for retrospectively testing whether the process was in control when the first
subgroups were being drawn. In this phase, the charts are used as aids to the practitioner in bringing a
process into a state where it is statistically in control.
\item Phase II: control charts are used for testing whether the process remains in control when future subgroups
are drawn. In this phase, the charts are used as aids to the practitioner in monitoring the process for any
change from an in-control state.
\end{itemize}

In short, phase I deals with estimating process parameters to ensure process stability using historical data, and phase II pertains to signal
any out‐of‐control condition or shifts in the process parameters.

Heckert et al (2002) introduce the $X$ control chart in detail. It is assumed that the probability distribution of the characteristic is the normal distribution. In phase I, m samples are required, let $d_{i}$ be the parameter ($s$) of the ith sample. Then the average of the parameter is


\begin{equation}
    \mu_{s}=\frac{1}{m} \sum_{i=1}^{m} d_{i}\,.
    \label{equ:mu}
\end{equation}

If $\sigma^{2}$ is the unknown variance of a probability distribution, then an unbiased estimator of $\sigma^{2}$ is the sample variance is

\begin{equation}
    s^{2}=\frac{\sum_{i=1}^{m}\left(d_{i}-\bar{x}\right)^{2}}{m-1}\,.
    \label{equ:sigma}
\end{equation}

However, $s$, the sample standard deviation, is not an unbiased estimator of $\sigma$. If the underlying distribution is normal, then $s$ is actually

\begin{equation}
    s=c_4 \times \sigma\,,
    \label{equ:s}
\end{equation}

where $c_4$ is 

\begin{equation}
    c_{4}=\sqrt{\frac{2}{m-1}} \frac{\left(\frac{m}{2}-1\right) !}{\left(\frac{m-1}{2}-1\right) !}\,,
    \label{equ:c4}
\end{equation}

in which $m$ is the sample size.

Finally, the standard deviation of the sample standard deviation is

\begin{equation}
    \sigma_{s}=\sigma \sqrt{1-c_{4}^{2}}\,.
    \label{equ:sigma_s}
\end{equation}

\subsection{Hotelling $T^{2}$ control chart}
In fact, most data in the industry (especially in chemistry and manufacturing)are naturally multivariate. Hotelling (1947) ~\nocite{hotteling1947multivariate}introduced a statistic that uniquely lends itself to plotting multivariate observations. This statistic, appropriately named Hotelling's $T^{2}$, is a scalar that combines information from the mean of several variables. We can image this scalar (so-called Hotelling $T^{2}$ distance) as a statistic describing the distance away from the mean. The derivation of Hotelling $T^{2}$ distance is as follows:

%\subsubsection{Multivariate normal distribution}
In univariate statistical quality control, we generally use the normal distribution to describe a continuous quality characteristic behavior. The univariate normal probability density function is


\begin{equation}
    f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}\,, \quad-\infty<x<\infty\,.
    \label{equ:normal_distribution}
\end{equation}

The mean of the normal distribution is $\mu$ and the variance is $\sigma$. The term (apart from the minus sign) in the exponent of the normal distribution can be written as follows:

\begin{equation}
    (x-\mu)\left(\sigma^{2}\right)^{-1}(x-\mu)\,.
    \label{equ:normal_distribution_part}
\end{equation}

This quantity measures the squared standardized distance from x to the mean $\mu$, whereby the term "standardized" means that the distance is expressed in standard deviation units. This same approach can be employed in the multivariate normal distribution case. 

Montgomery (2020) introduce the Hotelling $T^{2}$ control chart in detail. It is assumed that the joint probability distribution of the $p$ quality characteristics is the p-variate normal distribution. Suppose we have $p$ variables, given by $x_{1}, x_{2}, \ldots, x_{p}$. These variables are arranged in a p-component vector $\mathbf{x}^{\prime}=\left[x_{1}, x_{2}, \ldots, x_{p}\right]$. Let $\mu^{\prime}=\left[\mu_{1}, \mu_{2}, \ldots, \mu_{p}\right]$ be the vector of the means of the $x$'s, and let the variances and covariances of the random variables in $\mathbf{X}$ be contained in a $p × p$ covariance matrix $\Sigma$. The main diagonal elements of $\Sigma$ are the variances of the $x$’s and the off-diagonal elements are the covariances. Now the squared standardized distance from $\mathbf{x}$ to $\mu$ is

\begin{equation}
    (\mathbf{x}-\mu)^{\prime} \Sigma^{-1}(\mathbf{x}-\mu)\,.
    \label{equ:squared standardized (generalized) distance}
\end{equation}

Since we extend the concept of mu and sigma from univariate parameter into multivariate parameters, then our next step is naturally to figure out how to construct sample mean vector ($\overline{\mathbf{x}}$) and sample covariance matrix ($\mathbf{S}$) so that they can achieve the same effect as Shewhart $\bar{X}$ control chart.

Suppose that we have a random sample from a multivariate normal distribution
\begin{equation}
\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}\,,
\label{equ:x1_xn}
\end{equation}

where the ith sample vector contains observations on each of the $p$ variables 
$x_{i 1}, x_{i 2}, \ldots, x_{i p}$. Then the sample mean vector ($\overline{\mathbf{x}}$) is

\begin{equation}
\overline{\mathbf{x}}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{x}_{i}\,,
\label{equ:x_mean}
\end{equation}

and the sample covariance matrix ($\mathbf{S}$) is

\begin{equation}
\mathbf{S}=\frac{1}{n-1} \sum_{i=1}^{n}\left(\mathbf{x}_{i}-\overline{\mathbf{x}}\right)\left(\mathbf{x}_{i}-\overline{\mathbf{x}}\right)^{\prime}\,.
\label{equ:S_x}
\end{equation}

That is, the sample variances on the main diagonal of the matrix $\mathbf{S}$ are computed as

\begin{equation}
s_{j}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)^{2}\,,
\label{equ:S_j}
\end{equation}

and the sample covariances are

\begin{equation}
s_{j k}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)\left(x_{i k}-\bar{x}_{k}\right)\,.
\label{equ:S_ij}
\end{equation}

Now suppose that $\mathbf{S}$ from Equation (~\ref{equ:S_x}) is used to estimate
$\Sigma$ and that the vector ${\overline{\mathbf{X}}}$ is taken as the in-control value of the mean vector of the process. The Hotelling $T^{2}$ statistic is

\begin{equation}
T^{2}=n(\overline{\mathbf{x}}-\overline{\overline{\mathbf{x}}})^{\prime} \mathbf{S}^{-1}(\overline{\mathbf{x}}-\overline{\overline{\mathbf{x}}})\,.
\label{equ:T2_n}
\end{equation}

In which $n$ is the subgroup size of each sample.

In some industrial settings (e.g., chemical and process industries) the subgroup size is naturally $n = 1$. Then the Hotelling $T^{2}$ statistic in Equation
(\ref{equ:T2_n}) becomes

\begin{equation}
T^{2}=(\overline{\mathbf{x}}-\overline{\overline{\mathbf{x}}})^{\prime} \mathbf{S}^{-1}(\overline{\mathbf{x}}-\overline{\overline{\mathbf{x}}})\,.
\label{equ:T2}
\end{equation}

The phase II control limits for this statistic are
\begin{equation}
\begin{aligned}
\centering&\text { UCL }=\frac{p(m+1)(m-1)}{m^{2}-m p} F_{\alpha, p, m-p}\,, \\
\centering&\text { LCL }=0\,,
\label{equ:UCL_T2}
\end{aligned}
\end{equation}

in which $m$ is the sample size, $p$ is the number of quality characteristics, $\alpha$ is confident level.








































\begin{comment}
Firstly, let's consider the s chart, which is employed to determine if the distribution for the process characteristic is stable.
Suppose we have m preliminary samples at our disposition, each of size n, and let $s_i$ be the standard deviation of the ith sample. Then the average of the m standard deviations ($\bar{s}$) is
\begin{equation}
    \bar{s}=\frac{1}{m} \sum_{i=1}^{m} s_{i}
    \label{equ:s}
\end{equation}
\end{comment}




















